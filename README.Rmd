---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, echo = FALSE, message = FALSE,
  comment = "#>"
)
```

# hfx-bike-ridership

<!-- badges: start -->
<!-- badges: end -->

This is an end-to-end machine learning project to predict daily bike ridership in Halifax, Nova Scotia, Canada.

The model is deployed on Google Cloud Platform, and available as a [REST API](https://hfx-bike-ridership-api-74govvz7xq-uc.a.run.app/__docs__/) and a [Shiny dashboard](https://hfx-bike-ridership-app-74govvz7xq-uc.a.run.app).

For more information, check out this series of posts on my website:

1. [Part 1: getting the data.](https://tdunn.ca/posts/2022-04-27-predicting-bike-ridership-getting-the-data/)
2. [Part 2: develping a model.](https://tdunn.ca/posts/2022-04-29-predicting-bike-ridership-developing-a-model/)
3. [Part 3: deploying the model.](https://tdunn.ca/posts/2022-05-19-predicting-bike-ridership-deploying-the-model/)

## Model tuning

The ETL pipeline to extract the data, transform it, and load it into BigQuery, is currently scheduled to run on Sundays at midnight AST (or when I manually run it).
New data in BigQuery triggers Cloud Run to automatically re-train the model.

On the other hand, I decided to tune the XGBoost model locally instead of on GCP.
Tuning on my PC takes upwards of 10 minutes, even when running in parallel on a 6 core processor, so this was in the interest of keeping the project (relatively) free and avoiding any [expensive surprises](https://reddit.com/r/datascience/comments/tqe3y6/anyone_needs_ec2_instance/).

```{r}
library(tidyverse)
xgb_metrics <- readr::read_csv("model/tune/xgb-metrics.csv") %>%
  filter(timestamp == max(timestamp))
xgb_params <- readr::read_csv("model/tune/xgb-params.csv") %>%
  filter(timestamp == max(timestamp))
splits_resamples <- readr::read_csv("model/tune/splits-resamples.csv") %>%
  filter(timestamp == max(timestamp))
```

The code and results of the tuning are in the `model/tune/` folder of this repository.
The model was last tuned at `r unique(xgb_metrics$timestamp)` UTC.

The data were split `r 100 * splits_resamples$prop`-`r 100 * (1 - splits_resamples$prop)` by time.
The training set consisted of `r splits_resamples$n_train` observations, ranging in date from `r splits_resamples$min_date_train` to `r splits_resamples$max_date_train`, and the testing set consisted of `r splits_resamples$n_test` observations, ranging from `r splits_resamples$min_date_test + 1` to `r splits_resamples$max_date_test`.
A time-based resampling strategy was used (`rsample::sliding_period()`) to break the training set up into rolling periods of 13 months analysis and 1 month assessment.

The final hyperparameters, chosen by lowest mean absolute scaled error (MASE) were:

```{r}
xgb_params %>%
  select(-timestamp) %>%
  pivot_longer(cols = everything(), names_to = "parameter",
               values_to = "value", values_transform = as.character) %>%
  knitr::kable()
```

And the performance on the training set resamples and the test set:

```{r}
options(knitr.kable.NA = "")
xgb_metrics %>%
  select(-timestamp) %>%
  arrange(desc(data_set), metric) %>%
  mutate(value = round(value, 3), std_err = round(std_err, 3)) %>%
  knitr::kable()
```

